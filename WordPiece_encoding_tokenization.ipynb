{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WordPiece Tokenization"
      ],
      "metadata": {
        "id": "lyrq_URT7obw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a corpus of words\n",
        "corpus = [\n",
        "    \"This is the beginning of Hugging Face Course\",\n",
        "    \"This chapter is about tokenization\",\n",
        "    \"This section shows several tokenization algorithm\",\n",
        "    \"Hopefully you will be able to understand how they are trained to generate tokens\"\n",
        "]"
      ],
      "metadata": {
        "id": "YWs0QaTNMHlD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHw4eZdS4Lil",
        "outputId": "c84e7f62-685f-4fe0-af6d-8f2181939934"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "  words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "  new_words = [word for word, offset in words_with_offsets]\n",
        "  for word in new_words:\n",
        "    word_freqs[word]+=1\n",
        "\n",
        "print(word_freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hizHuML14UWB",
        "outputId": "c0793522-cfd5-4c4c-e8f5-bfef005606b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'beginning': 1, 'of': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, 'chapter': 1, 'about': 1, 'tokenization': 2, 'section': 1, 'shows': 1, 'several': 1, 'algorithm': 1, 'Hopefully': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 2, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'generate': 1, 'tokens': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = []\n",
        "for word in word_freqs.keys():\n",
        "  if word[0] not in alphabet:\n",
        "    alphabet.append(word[0])\n",
        "  for letter in word[1:]:\n",
        "    if f\"##{letter}\" not in alphabet:\n",
        "      alphabet.append(f\"##{letter}\")"
      ],
      "metadata": {
        "id": "pZJm86FM4VCo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jriDx0ijGRcx",
        "outputId": "2175b558-5f93-47d3-8e74-581934a002c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T', '##h', '##i', '##s', 'i', 't', '##e', 'b', '##g', '##n', 'o', '##f', 'H', '##u', 'F', '##a', '##c', 'C', '##o', '##r', 'c', '##p', '##t', 'a', '##b', '##k', '##z', 's', '##w', '##v', '##l', '##m', '##y', 'y', 'w', 'u', '##d', 'h', 'g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\",\"[MASK]\"] + alphabet.copy()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rtr2zp0KF9u",
        "outputId": "7dc90f78-0540-4286-dbb0-2893433d4183"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[PAD]',\n",
              " '[UNK]',\n",
              " '[CLS]',\n",
              " '[SEP]',\n",
              " '[MASK]',\n",
              " 'T',\n",
              " '##h',\n",
              " '##i',\n",
              " '##s',\n",
              " 'i',\n",
              " 't',\n",
              " '##e',\n",
              " 'b',\n",
              " '##g',\n",
              " '##n',\n",
              " 'o',\n",
              " '##f',\n",
              " 'H',\n",
              " '##u',\n",
              " 'F',\n",
              " '##a',\n",
              " '##c',\n",
              " 'C',\n",
              " '##o',\n",
              " '##r',\n",
              " 'c',\n",
              " '##p',\n",
              " '##t',\n",
              " 'a',\n",
              " '##b',\n",
              " '##k',\n",
              " '##z',\n",
              " 's',\n",
              " '##w',\n",
              " '##v',\n",
              " '##l',\n",
              " '##m',\n",
              " '##y',\n",
              " 'y',\n",
              " 'w',\n",
              " 'u',\n",
              " '##d',\n",
              " 'h',\n",
              " 'g']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {\n",
        "    word: [c if i==0 else f\"##{c}\" for i,c in enumerate(word)]\n",
        "    for word in word_freqs.keys()\n",
        "}"
      ],
      "metadata": {
        "id": "om9xqRqeKfD9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5FqyesRMHkf",
        "outputId": "a2e01002-7c09-4a5e-c8a7-b54720a54dcd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'This': ['T', '##h', '##i', '##s'],\n",
              " 'is': ['i', '##s'],\n",
              " 'the': ['t', '##h', '##e'],\n",
              " 'beginning': ['b', '##e', '##g', '##i', '##n', '##n', '##i', '##n', '##g'],\n",
              " 'of': ['o', '##f'],\n",
              " 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n",
              " 'Face': ['F', '##a', '##c', '##e'],\n",
              " 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n",
              " 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n",
              " 'about': ['a', '##b', '##o', '##u', '##t'],\n",
              " 'tokenization': ['t',\n",
              "  '##o',\n",
              "  '##k',\n",
              "  '##e',\n",
              "  '##n',\n",
              "  '##i',\n",
              "  '##z',\n",
              "  '##a',\n",
              "  '##t',\n",
              "  '##i',\n",
              "  '##o',\n",
              "  '##n'],\n",
              " 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n",
              " 'shows': ['s', '##h', '##o', '##w', '##s'],\n",
              " 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n",
              " 'algorithm': ['a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m'],\n",
              " 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n",
              " 'you': ['y', '##o', '##u'],\n",
              " 'will': ['w', '##i', '##l', '##l'],\n",
              " 'be': ['b', '##e'],\n",
              " 'able': ['a', '##b', '##l', '##e'],\n",
              " 'to': ['t', '##o'],\n",
              " 'understand': ['u',\n",
              "  '##n',\n",
              "  '##d',\n",
              "  '##e',\n",
              "  '##r',\n",
              "  '##s',\n",
              "  '##t',\n",
              "  '##a',\n",
              "  '##n',\n",
              "  '##d'],\n",
              " 'how': ['h', '##o', '##w'],\n",
              " 'they': ['t', '##h', '##e', '##y'],\n",
              " 'are': ['a', '##r', '##e'],\n",
              " 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n",
              " 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n",
              " 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pair_scores(splits):\n",
        "  letter_freqs = defaultdict(int)\n",
        "  pair_freqs = defaultdict(int)\n",
        "  for word, freq in word_freqs.items():\n",
        "    split = splits[word]\n",
        "    if len(split) == 1:\n",
        "      letter_freqs[split[0]] += freq\n",
        "      continue\n",
        "    for i in range(len(split)-1):\n",
        "      pair = (split[i], split[i+1])\n",
        "      letter_freqs[split[i]] += freq\n",
        "      pair_freqs[pair]+=freq\n",
        "    letter_freqs[split[-1]]+=freq\n",
        "\n",
        "    scores = {\n",
        "        pair: freq/(letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "        for pair, freq in pair_freqs.items()\n",
        "    }\n",
        "    return scores"
      ],
      "metadata": {
        "id": "T7ykBmmWMIHp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_scores = compute_pair_scores(splits)\n",
        "for i, key in enumerate(pair_scores.keys()):\n",
        "  print(f\"{key}: {pair_scores[key]}\")\n",
        "  if i>=5:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glIAJC3SOHWB",
        "outputId": "fd73f96e-284f-459c-83a4-d7143cd7f1df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T', '##h'): 0.3333333333333333\n",
            "('##h', '##i'): 0.3333333333333333\n",
            "('##i', '##s'): 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_pair = \"\"\n",
        "max_score = None\n",
        "for pair, score in pair_scores.items():\n",
        "  if max_score is None or max_score < score:\n",
        "    max_score = score\n",
        "    best_pair = pair\n",
        "\n",
        "print(best_pair, score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3QjBwy2OXYF",
        "outputId": "2216c1a7-00e5-49c1-b8a6-5d9f26e6daaf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T', '##h') 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7J4mGIQVkrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}